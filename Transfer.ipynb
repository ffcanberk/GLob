{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-win_amd64.whl (444.1 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.2)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorflow) (20.4)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorflow) (49.2.0.post20200714)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.23.2-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.34.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\fcanb\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.1.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=fd8f92fae3f515d658c25b2983062ab2cc04232a9840dbbd311567f9a68f6fce\n",
      "  Stored in directory: c:\\users\\fcanb\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: tensorflow-io-gcs-filesystem, flatbuffers, libclang, tensorflow-estimator, numpy, absl-py, tensorboard-data-server, grpcio, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, importlib-metadata, markdown, tensorboard-plugin-wit, tensorboard, keras-preprocessing, gast, keras, opt-einsum, astunparse, google-pasta, termcolor, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Users\\\\fcanb\\\\anaconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.5\n",
    "RANDOM_STATE = 2018\n",
    "BATCH_SIZE = 64\n",
    "NO_EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "SAMPLE_SIZE = 20000\n",
    "PATH = '/kaggle/input/dogs-vs-cats-redux-kernels-edition/'\n",
    "TRAIN_FOLDER = './train/'\n",
    "TEST_FOLDER =  './test/'\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(dropout=.25, learning_rate=0.001, augmentation=False):\n",
    "  # 'frozen' base model\n",
    "  base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "  base_model.trainable = False\n",
    "\n",
    "  # input layer\n",
    "  inputs = tf.keras.Input(shape=(299, 299, 3))\n",
    "  x = inputs\n",
    "  \n",
    "  # pass results to a preprocessing layer\n",
    "  x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
    "  \n",
    "  # send results to the already trained model\n",
    "  x = base_model(x, training=False)\n",
    "  \n",
    "  # send results to pooling layer\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  \n",
    "  # send to fully connected layer\n",
    "  x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "  \n",
    "  # send to dropout layer\n",
    "  x = tf.keras.layers.Dropout(dropout)(x)\n",
    "  \n",
    "  # send to batch normalization layer\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "   \n",
    "  # send to output dense layer with SoftMax activation function\n",
    "  outputs = tf.keras.layers.Dense(37,activation='softmax')(x)\n",
    "  \n",
    "  # create a model with these layers\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  \n",
    "  # show some information\n",
    "  model.summary()\n",
    "\n",
    "  # compile model with Adam optimizer\n",
    "  # and chosing accuracy as the metric to evaluate the model performance during training\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# setting image pixel side size\n",
    "IMG_SIZE = 299\n",
    "\n",
    "def getXy():\n",
    "  # function that returns the number correspondent to the breed of \n",
    "  # the animal in the image, given the image name\n",
    "  get_class_no = lambda name : info_by_breed[get_breed(name)]['globalid']\n",
    "  \n",
    "  # all image tensors will be stored here after resizing\n",
    "  training_data = []\n",
    "  \n",
    "  for img in all_imgs:\n",
    "    path = os.path.join(IMGS_PATH, img)\n",
    "  \n",
    "    # this is a trick to make the image be opened in RGB format, which is not the default\n",
    "    img_array = cv2.imread(path)[...,::-1] \n",
    "\n",
    "    # here the images are rezise\n",
    "    img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "  \n",
    "    # get the ID of the image class\n",
    "    class_no = get_class_no(img)\n",
    "  \n",
    "    training_data.append([img_array, class_no])\n",
    "    \n",
    "  # data should be in random order to improve performance\n",
    "  random.shuffle(training_data)\n",
    "  \n",
    "  # separating data from list\n",
    "  training = list(zip(*training_data))\n",
    "  X = training[0]\n",
    "  y = training[1]\n",
    "  \n",
    "  # transforming X to an np.array and resizing\n",
    "  X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "  y = np.array(y)\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one more import\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# stratified split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dictionary to save data\n",
    "lr = {}\n",
    "\n",
    "# lambda function to be passed in test_params function\n",
    "lr_model_func = lambda x : getModel(learning_rate=x)\n",
    "\n",
    "# we are testing only with 0.01 and 0.001\n",
    "for lr_val in [0.01, 0.001]:\n",
    "  # for each value, call function and save data in dictionary\n",
    "  lr[lr_val] = test_params(lr_val, lr_model_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
